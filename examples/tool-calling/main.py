#!/usr/bin/env python

import json
from itertools import count

from langchain.messages import HumanMessage, SystemMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI


def dump_messages(messages):
    """Return all messages as dicts, stripping out any non-deterministic data
    (that was not generated by the LLM) to ensure bitwise reproducibility of the
    logged output.
    """
    return [
        message.model_dump()
        | {
            "id": "",  # Randomly generated message ID
            "response_metadata": message.response_metadata
            | {
                "id": "",  # Another randomly generated ID
                "system_fingerprint": "",  # Probably machine-specific
            },
        }
        for message in messages
    ]


class ToolCallingAgent:
    """Minimal implementation of a tool-calling agent.

    Not using any of LangChain's agent implementations here due to partial
    incompatibility with llama.cpp's implementation of the OpenAI API.

    Args:
        llm: The language model to use for generating responses and deciding which
            tools to call.
        tools: The tools that the agent can use.
        system_message: The system message to use for the agent's conversation.
    """

    def __init__(self, llm, tools, system_message):
        self.llm = llm
        self.llm_with_tools = llm.bind_tools(tools)
        self.tools = {tool.func.__name__: tool for tool in tools}
        self.messages = [SystemMessage(system_message)]
        self.tool_call_id_counter = count()  # For deterministic tool call IDs

    def invoke(self, query):
        """Invoke the agent with a user query."""
        # User asks a question
        human_message = HumanMessage(query)
        self.messages.append(human_message)

        # LLM responds with tool call
        ai_message = self.llm_with_tools.invoke(self.messages)

        # Replace random tool call IDs generated by llama.cpp in its response with
        # deterministic ones for LLM output reproducibility, as these are passed
        # back to the LLM in the next step and therefore influence its output.
        # However, we still want them to be unique within a single run, so that each
        # tool message can uniquely reference the tool call that triggered it.
        ai_message.tool_calls = [
            call | {"id": str(next(self.tool_call_id_counter))}
            for call in ai_message.tool_calls
        ]
        self.messages.append(ai_message)

        # We invoke the tools that were called
        for tool_call in ai_message.tool_calls:
            tool = self.tools[tool_call["name"]]
            tool_message = tool.invoke(tool_call)
            self.messages.append(tool_message)

        # LLM provides final answer
        ai_message_final = self.llm.invoke(self.messages)
        self.messages.append(ai_message_final)


@tool
def get_datetime() -> str:
    """Get the current time."""
    # We can't return the actual current time here because that would make the test
    # non-deterministic.
    return "2026-01-27T14:32:33.990923"


@tool
def get_weather(location: str) -> str:
    """Get the current weather in a given location.

    Args:
        location: The location to get the weather for.
    """
    if location.lower() == "san francisco":
        return "Sunny, 72F"
    elif location.lower() == "new york":
        return "Cloudy, 60F"
    else:
        return "Rainy, 55F"


def main():
    llm = ChatOpenAI(base_url="http://localhost:8080/v1", api_key="")
    system_message = (
        "You are a helpful assistant. You can use tools to get the current time and "
        "weather information. Once you have the information, provide an answer to "
        "user."
    )
    agent = ToolCallingAgent(llm, [get_datetime, get_weather], system_message)
    agent.invoke("What time is it?")
    agent.invoke("What's the weather in San Francisco?")
    print(json.dumps(dump_messages(agent.messages), indent=2))


if __name__ == "__main__":
    main()
